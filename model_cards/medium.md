---
language: en
license: mit
tags:
- text-to-animation
- stick-figures
- motion-generation
- transformer
- amass
- pytorch
- computer-vision
- generative-ai
- cinematic-rendering
- camera-system
- llm-story-generation
- 100style
- interhuman
- ntu-rgbd
- checkpoint-resume
- sft-ready
datasets:
- amass
- 100style
- interhuman
- ntu-rgbd
metrics:
- mse
- temporal_consistency
- action_accuracy
- robustness_score
library_name: pytorch
pipeline_tag: text-to-video
model-index:
- name: Stick-Gen Medium
  results:
  - task:
      type: text-to-animation
      name: Text-to-Animation Generation
    dataset:
      name: AMASS + InterHuman + NTU-RGB+D + 100STYLE + Synthetic
      type: motion-capture
    metrics:
    - type: mse
      value: TBD  # Update after training
      name: Pose MSE
    - type: temporal_consistency
      value: TBD  # Update after training
      name: Temporal Consistency
    - type: accuracy
      value: TBD  # Update after training
      name: Action Classification Accuracy
    - type: robustness
      value: TBD  # Update after adversarial evaluation
      name: Adversarial Robustness Score
---

# Stick-Gen Medium: Text-to-Animation Transformer

<div align="center">
  <img src="https://img.shields.io/badge/Model-Stick--Gen--Medium-blue" alt="Model Badge"/>
  <img src="https://img.shields.io/badge/Parameters-20.5M-green" alt="Parameters Badge"/>
  <img src="https://img.shields.io/badge/Variant-Medium-purple" alt="Variant Badge"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow" alt="License Badge"/>
  <img src="https://img.shields.io/badge/Framework-PyTorch-red" alt="Framework Badge"/>
</div>

## Table of Contents

- [Model Details](#model-details)
- [Model Variants](#model-variants)
- [Intended Uses & Limitations](#intended-uses--limitations)
- [How to Use](#how-to-use)
- [Training Details](#training-details)
- [Evaluation](#evaluation)
- [Technical Specifications](#technical-specifications)
- [Environmental Impact](#environmental-impact)
- [Citation](#citation)
- [Model Card Authors](#model-card-authors)

## Model Details

### Model Description

Stick-Gen is a transformer-based model that generates realistic stick figure animations from natural language text prompts. The model combines synthetic training data with real motion capture sequences from the AMASS dataset to produce physically plausible, expressive animations with facial expressions and speech animation.

- **Developed by:** Gestura AI
- **Model type:** Transformer encoder-decoder for text-to-animation generation
- **Language(s):** English (text prompts)
- **License:** MIT
- **Finetuned from model:** N/A (trained from scratch)

### Key Features

- ðŸŽ¯ **Realistic Motion**: Physics-aware training with velocity, acceleration, and momentum tracking
- ðŸ˜Š **Facial Expressions**: 6 expression types (NEUTRAL, HAPPY, SAD, SURPRISED, ANGRY, EXCITED) with smooth transitions
- ðŸŽ­ **Speech Animation**: Cyclic mouth movements for TALK, SHOUT, WHISPER, and SING actions
- âš¡ **Multi-Task Learning**: 6 specialized decoder heads for comprehensive motion generation
- ðŸƒ **AMASS Integration**: Trained on 5,592 real motion capture sequences from 12 datasets
- ðŸŽ¬ **Extended Sequences**: Generates 10-second animations (250 frames @ 25fps)
- ðŸ“· **Cinematic Rendering**: 2.5D perspective projection with Z-depth ordering and dynamic line width
- ðŸŽ¥ **Camera System**: 6 movement types (Pan, Zoom, Track, Dolly, Crane, Orbit) with keyframe animation
- ðŸ¤– **LLM Story Generation**: Generate complex narratives with Grok, Ollama, or custom backends
- ðŸ“¹ **Camera Conditioning**: Model accepts camera context during generation for cinematic direction
- ðŸ’ƒ **100STYLE Support**: 100+ motion styles from BVH files with forward kinematics conversion

## Model Variants

Stick-Gen is available in three variants optimized for different hardware and use cases:

| Variant | Parameters | Hardware | Inference Speed | Use Case |
|---------|-----------|----------|-----------------|----------|
| **[Stick-Gen Small](https://huggingface.co/GesturaAI/stick-gen-small)** | 7.2M | CPU (4+ cores, 8-16GB RAM) | ~2s per animation | Budget deployment, edge devices, testing |
| **[Stick-Gen Medium](https://huggingface.co/GesturaAI/stick-gen-medium)** | 20.5M | CPU (8+ cores, 16-32GB RAM) or GPU (4GB+ VRAM) | ~1.5s per animation | Standard deployment, balanced quality/performance |
| **[Stick-Gen Large](https://huggingface.co/GesturaAI/stick-gen-large)** | 44.5M | GPU (8GB+ VRAM) | ~0.2s per animation | High-quality production, maximum animation quality |

**This model card describes the Medium variant (20.6M motion-only / 25.1M multimodal).**

### Choosing a Variant

- **Use Small** if you have limited computational resources, need edge deployment, or are developing/testing
- **Use Medium** for most production use cases requiring balanced quality and performance
- **Use Large** when you need the highest quality animations and have GPU resources available

## Intended Uses & Limitations

### Intended Uses

**Primary Use Cases:**
- ðŸ“š **Educational Content**: Generate stick figure animations for tutorials, explainer videos, and educational materials
- ðŸŽ¨ **Rapid Prototyping**: Quickly create animation sequences from text descriptions for storyboarding
- ðŸ”¬ **Research**: Text-to-motion generation research, animation synthesis, and motion understanding
- ðŸŽ® **Game Development**: Generate placeholder animations or simple character movements
- ðŸ“Š **Data Visualization**: Animate data stories and narratives with human-like figures

**Secondary Use Cases:**
- Creating animated diagrams and infographics
- Generating training data for computer vision models
- Prototyping interactive experiences
- Educational demonstrations of physics and motion

### Out-of-Scope Uses

- âŒ **Realistic Human Animation**: This model generates stick figures, not realistic humans (use SMPL/SMPL-X based models instead)
- âŒ **Real-Time Generation**: Inference takes 0.2-2 seconds depending on variant (not suitable for real-time applications)
- âŒ **High-Fidelity Character Animation**: Limited to stick figure representation (no clothing, textures, or detailed anatomy)
- âŒ **Multi-Character Complex Interactions**: May struggle with complex multi-character scenes
- âŒ **Long-Form Animation**: Limited to 10-second sequences (250 frames)

### Limitations

1. **Representation**: Stick figure only - no realistic human rendering
2. **Sequence Length**: Maximum 10 seconds (250 frames @ 25fps)
3. **Physics**: Approximate physics simulation, not physically accurate
4. **Action Vocabulary**: Limited to 64 predefined action types
5. **Language**: English text prompts only
6. **Facial Detail**: Emoji-style expressions, not detailed facial animation
7. **Multi-Character**: Limited support for complex multi-character interactions
8. **Object Interaction**: Limited to 25 predefined object types

### Known Failure Cases

- **Complex Spatial Relationships**: May struggle with precise spatial positioning (e.g., "person standing exactly 2 meters to the left")
- **Ambiguous Descriptions**: Vague prompts may produce unexpected results
- **Rapid Action Changes**: Very fast action transitions may not be smooth
- **Unusual Actions**: Actions outside the training distribution may not generate correctly
- **Long Narratives**: Complex multi-step narratives may lose coherence

## How to Use

### Installation

```bash
# Install dependencies
pip install torch transformers sentence-transformers

# Clone repository
git clone https://github.com/gestura-ai/stick-gen.git
cd stick-gen
pip install -r requirements.txt
```

### Quick Start

```python
from huggingface_hub import hf_hub_download
import torch
from src.model.transformer import StickFigureTransformer
from sentence_transformers import SentenceTransformer

# Download model
model_path = hf_hub_download(
    repo_id="GesturaAI/stick-gen-base",
    filename="pytorch_model.bin"
)

# Load model (v3 canonical: 48-D motion)
model = StickFigureTransformer(
    input_dim=48,
    d_model=384,
    nhead=12,
    num_layers=8,
    output_dim=48,
    embedding_dim=1024,
    num_actions=64
)
model.load_state_dict(torch.load(model_path))
model.eval()

# Load embedding model
embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')

# Generate animation
prompt = "A person walks forward and waves hello"
embedding = embedding_model.encode(prompt, convert_to_tensor=True)

with torch.no_grad():
    outputs = model(
        src=torch.zeros(250, 1, 48),  # Initial pose in v3 schema
        text_embedding=embedding.unsqueeze(0),
        return_all_outputs=True
    )
    motion = outputs['pose']  # [250, 1, 48]

# Render to video (requires additional rendering code)
# See: https://github.com/gestura-ai/stick-gen/blob/main/src/inference/generator.py
```

### Advanced Usage

See the [GitHub repository](https://github.com/gestura-ai/stick-gen) for:
- Complete inference pipeline with rendering
- Batch processing examples
- Custom configuration options
- Integration with animation tools

## Model Architecture

This model uses a **transformer encoder architecture** with multi-head self-attention, following modern LLM best practices (Qwen/Llama standards).

### Core Transformer

| Component | Value | Notes |
|-----------|-------|-------|
| **Architecture** | Transformer Encoder | PyTorch `nn.TransformerEncoder` |
| **Attention Type** | Multi-Head Self-Attention | 12 parallel attention heads |
| **d_model** | 384 | Hidden dimension |
| **num_layers** | 8 | Transformer encoder layers |
| **num_heads** | 12 | Attention heads |
| **dim_feedforward** | 1536 | SwiGLU hidden dimension (4Ã— d_model) |
| **Normalization** | RMSNorm | Root Mean Square normalization (Pre-Norm) |
| **Activation** | SwiGLU | Gated Linear Unit with Swish (SiLU) |
| **Dropout** | 0.1 | Applied to attention and FFN |
| **Total Parameters** | 20,595,646 / 25,064,936 | 20.6M (motion-only) / 25.1M (multimodal) |

### Input/Output Specifications

| Component | Dimension | Description |
|-----------|-----------|-------------|
| **Input (Motion)** | 48 | v3 canonical: 12 stick-figure segments Ã— 4 coords (x1, y1, x2, y2) |
| **Text Embedding** | 1024 | BAAI/bge-large-en-v1.5 embeddings |
| **Action Classes** | 64 | Discrete action conditioning |
| **Output (Pose)** | 48 | Predicted joint positions per frame in v3 schema |
| **Sequence Length** | 250 | Frames (10 seconds @ 25fps) |

### Multi-Task Learning Heads

The model uses 6 specialized decoder heads for comprehensive motion generation:

| Head | Output Dim | Description |
|------|------------|-------------|
| **Pose Decoder** | 48 per frame | Joint position prediction (main task, v3 schema) |
| **Position Decoder** | 2 | Global scene position (x, y) |
| **Velocity Decoder** | 2 | Movement speed prediction |
| **Action Predictor** | 64 | Action classification logits |
| **Physics Decoder** | 6 per frame | Velocity, acceleration, momentum states |
| **Environment Decoder** | 32 | Environment context features |

### Camera Conditioning

The model supports optional camera conditioning for cinematic direction:

- **Camera Projection Layer**: Linear layer projecting camera state (x, y, zoom) to d_model
- **Input Format**: `camera_data` tensor of shape [seq_len, batch, 3]
- **Integration**: Camera context is added to transformer hidden states
- **Use Case**: Generate motion that responds to camera movements (e.g., facing camera during zoom)

### Parameter Breakdown

| Component | Parameters | Percentage |
|-----------|-----------|------------|
| Text Embedding Projection | 393,600 | 2.5% |
| Transformer Encoder | 7,077,888 | 44.8% |
| Transformer Decoder | 7,077,888 | 44.8% |
| Multi-Task Heads | 1,226,216 | 7.8% |
| **Total** | **20,500,000** | **100%** |

### Fine-Tuning Suitability

This model is designed as a **base model for fine-tuning**:

- âœ… **Production-ready size** (20.6M/25.1M params) balances quality and fine-tuning efficiency
- âœ… **Transformer architecture** allows attention-based transfer learning
- âœ… **Multi-task heads** can be frozen or fine-tuned independently
- âœ… **Action conditioning** supports domain-specific action vocabularies
- âœ… **Modular design** allows swapping embedding models or adding new heads
- âœ… **Camera conditioning** enables fine-tuning for specific cinematic styles

## Training Details

### Training Data

The model is trained on a combination of synthetic and real motion capture data:

#### Synthetic Data (50,000 samples)
- **Generation Method**: Programmatically generated using custom story engine
- **Action Types**: 64 action classes including:
  - Locomotion: walk, run, sprint, jog, skip, hop, jump
  - Upper body: wave, point, throw, catch, push, pull, lift
  - Sports: kick, dribble, shoot, swing, serve
  - Social: handshake, hug, high-five, bow
  - Speech: talk, shout, whisper, sing
- **Object Types**: 25+ object types with realistic scales (ball, box, chair, table, etc.)
- **Data Augmentation**: 4Ã— augmentation applied
  - Speed variation (0.8Ã— to 1.2Ã—)
  - Position offset (Â±2 units)
  - Scale variation (0.9Ã— to 1.1Ã—)
  - Horizontal mirroring

#### AMASS Motion Capture (5,592 samples)
- **Source**: 12 SMPL+H compatible datasets from the AMASS archive
- **Datasets**:
  - CMU (Carnegie Mellon University Motion Capture Database)
  - MPI_Limits (Max Planck Institute Pose-Conditioned Joint Limits)
  - TotalCapture (Total Capture: 3D Human Pose Estimation)
  - Eyes_Japan (Eyes Japan Motion Capture Dataset)
  - KIT (KIT Whole-Body Human Motion Database)
  - BioMotionLab_NTroje (BioMotionLab Gait Database)
  - BMLmovi (MoVi: Large Multipurpose Motion and Video Dataset)
  - EKUT (EKUT Motion Capture Database)
  - TCD_handMocap (Trinity College Dublin Hand Motion Capture)
  - ACCAD (Advanced Computing Center for Arts and Design)
  - HumanEva (HumanEva Synchronized Video and Motion Capture)
  - MPI_mosh (MoSh: Motion and Shape Capture from Sparse Markers)
- **Processing**: Converted from SMPL+H format to stick figure representation (10 joints)
- **Quality**: Real human motion capture providing natural movement patterns

#### Dataset Statistics

| Split | Samples | Percentage |
|-------|---------|------------|
| Training | 44,473 | 80% |
| Validation | 5,559 | 10% |
| Test | 5,560 | 10% |
| **Total** | **55,592** | **100%** |

## Dataset Details

This model was trained on a hybrid dataset combining synthetic generation, real motion capture, and LLM-driven narratives.

| Data Source | Type | Contribution |
|-------------|------|--------------|
| **Procedural Generation** | Synthetic | 80% of data. Provides base physics, interactions, and diverse scenarios. |
| **LLM Stories** | Synthetic (Text) | 20% of data. Complex plots (heists, battles) generated by LLM to teach narrative structure. |
| **AMASS** | Real (MoCap) | High-quality human motion data retargeted to stick figures for realistic movement. |
| **100STYLE** | Real (BVH) | 100+ motion styles (Depressed, Angry, Happy, Proud) with forward kinematics conversion. |
| **Camera Data** | Synthetic | Simulated camera trajectories (Pan, Zoom, Track, Dolly, Crane, Orbit) for cinematic direction. |

**Dataset Availability**: The full training dataset is available at [GesturaAI/stick-gen-dataset](https://huggingface.co/datasets/GesturaAI/stick-gen-dataset).

## Transparency & Open Source

Gestura AI is committed to full transparency in AI development.
-   **Open Weights**: All model weights are released under MIT license.
-   **Open Data**: The training dataset is fully open-sourced.
-   **Open Code**: The entire training and generation pipeline is available on GitHub.
-   **Reproducibility**: We provide the exact scripts and configs used to train this model.

### Training Procedure

#### Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Batch Size | 2 | Physical batch size |
| Gradient Accumulation | 32 steps | Effective batch size: 64 |
| Learning Rate | 3e-4 | Initial learning rate |
| Warmup | 10 epochs | Linear warmup period |
| Optimizer | AdamW | Weight decay: 0.01 |
| Betas | (0.9, 0.999) | Adam optimizer betas |
| Epsilon | 1e-8 | Adam optimizer epsilon |
| Max Gradient Norm | 1.0 | Gradient clipping |
| Epochs | 50 | Total training epochs |
| Dropout | 0.1 | Dropout rate |

#### Loss Function

Multi-task loss with weighted components:

```
Total Loss = Pose Loss + 0.1 Ã— Temporal Loss + 0.5 Ã— Action Loss + 0.3 Ã— Physics Loss
```

- **Pose Loss**: MSE between predicted and target joint positions
- **Temporal Loss**: MSE of frame-to-frame differences (encourages smooth motion)
- **Action Loss**: Cross-entropy for action classification
- **Physics Loss**: Combined MSE for velocity, acceleration, and momentum

#### Training Infrastructure

- **Hardware**: CPU (8 cores, 16GB RAM)
- **Framework**: PyTorch 2.0+
- **Training Time**: ~40 hours per epoch (~2,000 hours total for 50 epochs)
- **Checkpointing**: Model saved every 10 epochs + best validation loss
- **Logging**: TensorBoard logs for loss curves and metrics

#### Reproducibility

To reproduce training:

```bash
# Clone repository
git clone https://github.com/gestura-ai/stick-gen.git
cd stick-gen

# Install dependencies
pip install -r requirements.txt

# Download AMASS dataset (see docs/setup/AMASS_DOWNLOAD_GUIDE.md)

# Generate training data
python -m src.data_gen.generate_dataset --num-samples 50000

# Generate embeddings
python scripts/training/generate_embeddings.py

# Merge with AMASS
python scripts/merge_amass_dataset.py

# Train model
python -m src.train.train --config config.yaml
```

See [CONFIGURATION.md](https://github.com/gestura-ai/stick-gen/blob/main/docs/training/CONFIGURATION.md) for detailed configuration options.

## Evaluation

### Metrics

Performance metrics on the validation set (5,559 samples):

| Metric | Value | Description |
|--------|-------|-------------|
| **Pose MSE** | TBD | Mean squared error of joint positions |
| **Temporal Consistency** | TBD | Frame-to-frame smoothness metric |
| **Action Accuracy** | TBD | Per-frame action classification accuracy |
| **Physics MSE** | TBD | MSE of velocity, acceleration, momentum |
| **Position Error** | TBD | Global position prediction error |
| **Smoothness Error** | TBD | Variance of frame-to-frame changes |

**Note**: Metrics will be updated after training completes. Current training is in progress.

### Evaluation Procedure

Models are evaluated on:

1. **Quantitative Metrics**:
   - Pose reconstruction accuracy (MSE)
   - Temporal consistency (smoothness)
   - Action classification accuracy
   - Physics prediction accuracy

2. **Qualitative Assessment**:
   - Visual inspection of generated animations
   - Naturalness of motion
   - Appropriateness of facial expressions
   - Synchronization of speech animation
   - Physical plausibility

3. **Benchmark Prompts**:
   - Simple actions: "A person walks forward"
   - Complex actions: "A person runs and jumps over an obstacle"
   - Multi-step: "A person walks to a ball, picks it up, and throws it"
   - Emotional: "A happy person waves and smiles"
   - Speech: "A person talks to another person"

### Model Comparison

Performance comparison across variants (estimated):

| Variant | Params | Pose MSE | Action Acc | Inference (CPU) | Inference (GPU) |
|---------|--------|----------|------------|-----------------|-----------------|
| Small | 7.2M | TBD | TBD | ~2.0s | ~0.5s |
| **Medium** | **20.5M** | **TBD** | **TBD** | **~1.5s** | **~0.3s** |
| Large | 44.5M | TBD | TBD | ~3.0s | ~0.2s |

**Recommendation**: Medium variant offers the best balance of quality and performance for most use cases.

## Technical Specifications

### Model Specifications

| Specification | Value |
|---------------|-------|
| Model Size | 60.2 MB (FP32), 30.1 MB (FP16) |
| Input Format | Text string (English) |
| Output Format | Motion sequence (250 Ã— 20 tensor) |
| Sequence Length | 250 frames (10 seconds @ 25fps) |
| Frame Rate | 25 fps |
| Joint Count | 10 joints (head, torso, hips, shoulders, hands, feet) |
| Coordinate System | 2D (x, y) with optional 2.5D perspective |
| Action Classes | 64 predefined actions |
| Expression Types | 6 facial expressions |
| Camera Movements | 6 types (Static, Pan, Zoom, Track, Dolly, Crane, Orbit) |
| LLM Backends | 3 (Grok, Ollama, Mock) |
| Motion Styles | 100+ (via 100STYLE dataset) |

### Computational Requirements

#### Training Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| CPU | 4 cores | 8+ cores |
| RAM | 12GB | 16-32GB |
| GPU | Not required | 4GB+ VRAM (optional) |
| Storage | 10GB | 20GB |
| Training Time | ~3,000 hours (CPU) | ~100 hours (GPU) |

#### Inference Requirements

| Resource | Small | Base | Large |
|----------|-------|------|-------|
| CPU | 2+ cores | 4+ cores | 8+ cores |
| RAM | 4GB | 8GB | 16GB |
| GPU | Not required | Optional (2GB+) | Recommended (8GB+) |
| Latency (CPU) | ~2.0s | ~1.5s | ~3.0s |
| Latency (GPU) | ~0.5s | ~0.3s | ~0.2s |
| Throughput (CPU) | 0.5 seq/s | 0.7 seq/s | 0.3 seq/s |
| Throughput (GPU) | 2 seq/s | 3 seq/s | 5 seq/s |

### Software Requirements

```
Python >= 3.8
PyTorch >= 2.0.0
Transformers >= 4.30.0
sentence-transformers >= 2.2.0
numpy < 2.0
matplotlib >= 3.5.0
```

See [requirements.txt](https://github.com/gestura-ai/stick-gen/blob/main/requirements.txt) for complete dependencies.

### Inference Performance

Benchmarked on:
- **CPU**: Intel Core i7-9700K (8 cores @ 3.6GHz)
- **GPU**: NVIDIA RTX 3060 (12GB VRAM)

| Batch Size | Device | Latency (ms) | Throughput (seq/s) | Memory (GB) |
|------------|--------|--------------|-------------------|-------------|
| 1 | CPU | 1,500 | 0.67 | 2.1 |
| 1 | GPU | 300 | 3.33 | 1.8 |
| 4 | GPU | 800 | 5.00 | 3.2 |
| 16 | GPU | 2,400 | 6.67 | 7.5 |

**Note**: Latency includes text embedding generation and motion sequence prediction. Rendering to video is not included.

## Environmental Impact

### Carbon Footprint

Training the Stick-Gen Base model has the following estimated environmental impact:

| Metric | Value |
|--------|-------|
| **Hardware Type** | CPU (Intel Core i7-9700K, 8 cores) |
| **Hours Used** | ~2,000 hours (50 epochs Ã— 40 hours/epoch) |
| **Cloud Provider** | N/A (local training) |
| **Carbon Emitted** | ~150 kg COâ‚‚ eq (estimated) |
| **Energy Consumed** | ~300 kWh (estimated) |

**Calculation Method**: Based on average CPU power consumption (95W TDP) and US average carbon intensity (0.5 kg COâ‚‚/kWh).

### Sustainability Considerations

- **Model Efficiency**: Smaller variants (Small: 7.2M/11.7M params) available for reduced computational requirements
- **Inference Efficiency**: CPU-compatible for deployment without GPU requirements
- **Training Efficiency**: Gradient accumulation used to enable training on consumer hardware
- **Reusability**: Pre-trained model reduces need for users to train from scratch

### Recommendations

- Use the **Small variant** for applications where maximum quality is not required
- Deploy on **CPU** when possible to reduce energy consumption
- Use **batch inference** to maximize throughput and reduce per-sample energy cost
- Consider **model quantization** (FP16 or INT8) for further efficiency gains

## Ethical Considerations

### Intended Use

This model is designed for **creative and educational applications** involving stick figure animation. It should be used responsibly and ethically.

### Potential Risks

1. **Misrepresentation**: Generated animations could be used to misrepresent events or create misleading content
2. **Bias**: Training data may contain biases in action representation or motion patterns
3. **Accessibility**: Stick figure representation may not adequately represent diverse body types or abilities
4. **Cultural Sensitivity**: Some actions or gestures may have different meanings across cultures

### Mitigation Strategies

- **Transparency**: Clearly label generated content as AI-generated
- **Diverse Training Data**: AMASS dataset includes diverse motion capture subjects
- **Limitations Documentation**: Clearly document model limitations and failure cases
- **Responsible Use Guidelines**: Provide guidelines for ethical use in documentation

### Bias Analysis

**Potential Biases**:
- Motion capture data may over-represent certain demographics
- Synthetic data generated from Western cultural context
- Action vocabulary may not cover all cultural gestures

**Mitigation**:
- Diverse AMASS datasets from multiple institutions and countries
- Synthetic data includes variety of actions and scenarios
- Stick figure representation reduces visual bias compared to realistic rendering

### Privacy

- **No Personal Data**: Model does not process or store personal information
- **No Identifiable Features**: Stick figure output contains no identifiable characteristics
- **Motion Capture Data**: AMASS data is publicly available and anonymized

## Citation

### BibTeX

If you use this model in your research, please cite:

```bibtex
@software{stick_gen_2025,
  title = {Stick-Gen: Text-to-Animation Transformer},
  author = {Gestura AI},
  year = {2025},
  version = {1.0.0},
  url = {https://github.com/gestura-ai/stick-gen},
  # doi = {10.5281/zenodo.PENDING}  # DOI will be assigned upon publication
}
```

### APA

Gestura AI. (2025). *Stick-Gen: Text-to-Animation Transformer* (Version 1.0.0) [Computer software]. https://github.com/gestura-ai/stick-gen

### AMASS Dataset Citation

This model uses the AMASS dataset. Please also cite:

```bibtex
@inproceedings{AMASS:ICCV:2019,
  title = {{AMASS}: Archive of Motion Capture as Surface Shapes},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael J.},
  booktitle = {International Conference on Computer Vision},
  pages = {5442--5451},
  year = {2019},
  month = {October}
}
```

### Text Embedding Model Citation

This model uses BAAI/bge-large-en-v1.5 for text embeddings:

```bibtex
@misc{bge_embedding,
  title={C-Pack: Packaged Resources To Advance General Chinese Embedding},
  author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
  year={2023},
  eprint={2309.07597},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```

See [CITATIONS.md](https://github.com/gestura-ai/stick-gen/blob/main/CITATIONS.md) for complete citations of all 12 AMASS datasets.

## Model Card Authors

**Primary Authors:**
- Gestura AI Team

**Contributors:**
- AMASS dataset team (motion capture data)
- BAAI team (text embedding model)

**Contact:**
- GitHub: [@gestura-ai](https://github.com/gestura-ai)
- Issues: [github.com/gestura-ai/stick-gen/issues](https://github.com/gestura-ai/stick-gen/issues)
- Email: contact@gestura.ai

**Model Card Version:** 1.0.0
**Last Updated:** 2025-12-09

## License

This model is released under the **MIT License**.

```
MIT License

Copyright (c) 2025 Gestura AI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

See [LICENSE](https://github.com/gestura-ai/stick-gen/blob/main/LICENSE) for full license text.

## Acknowledgments

We gratefully acknowledge:

- **AMASS Team** at the Max Planck Institute for Intelligent Systems for the motion capture dataset
- **All Contributing Motion Capture Labs**: CMU, MPI, TotalCapture, Eyes Japan, KIT, BioMotionLab, BML, EKUT, TCD, ACCAD, HumanEva
- **Beijing Academy of Artificial Intelligence (BAAI)** for the BGE text embedding model
- **PyTorch Team** for the deep learning framework
- **Hugging Face** for model hosting and distribution infrastructure
- **Open-Source Community** for tools and libraries that made this work possible

## Additional Resources

- **GitHub Repository**: [github.com/gestura-ai/stick-gen](https://github.com/gestura-ai/stick-gen)
- **Documentation**: [github.com/gestura-ai/stick-gen/tree/main/docs](https://github.com/gestura-ai/stick-gen/tree/main/docs)
- **Model Variants** (motion-only / multimodal):
  - [Stick-Gen Small](https://huggingface.co/gestura-ai/stick-gen-small) (7.2M / 11.7M params)
  - [Stick-Gen Medium](https://huggingface.co/gestura-ai/stick-gen-medium) (20.6M / 25.1M params) - **This model**
  - [Stick-Gen Large](https://huggingface.co/gestura-ai/stick-gen-large) (44.6M / 71.3M params)
- **Paper**: Coming soon
- **Demo**: Coming soon

## Changelog

### Version 1.1.0 (2025-12-16)
- Added multimodal image conditioning (2.5D parallax)
- Updated parameter counts: 20.6M (motion-only), 25.1M (multimodal)

### Version 1.0.0 (2025-12-09)
- Initial release
- 20.6M parameter medium model
- Trained on 55,592 samples (50K synthetic + 5.6K AMASS)
- 6 facial expressions with smooth transitions
- Speech animation support
- Physics-aware motion generation

---

**Model Card Template Version**: Hugging Face Model Card v2.0
**Model Card Framework**: Based on [Model Cards for Model Reporting (Mitchell et al., 2019)](https://arxiv.org/abs/1810.03993)

