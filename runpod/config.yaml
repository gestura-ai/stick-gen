# Stick-Gen RunPod Endpoint Configuration
# Gestura AI - https://gestura.ai

# Endpoint Settings
endpoint:
  name: "stick-gen"
  description: "Text-to-Animation Stick Figure Generator by Gestura AI"
  
# Docker Image (GitHub Container Registry)
docker:
  image: "gestura-ai/stick-gen:latest"
  registry: "ghcr.io"
  # Full image path: ghcr.io/gestura-ai/stick-gen:latest
  # After first push, make public at: https://github.com/orgs/gestura-ai/packages/container/stick-gen/settings

# GPU Configuration
gpu:
  # Recommended: RTX 3090, RTX 4090, A100
  type: "NVIDIA RTX A4000"  # Good balance of cost/performance
  count: 1
  min_vram_gb: 16

# Worker Settings
worker:
  min_workers: 0           # Scale to zero when idle
  max_workers: 3           # Maximum concurrent workers
  idle_timeout: 300        # Seconds before scaling down (5 min)
  
# Environment Variables
environment:
  MODEL_VARIANT: "medium"  # Options: small, medium, large
  MODEL_PATH: "/workspace/models/model_checkpoint.pth"
  CONFIG_PATH: "/workspace/configs/medium.yaml"
  DEVICE: "cuda"
  # Grok API Key (referenced from RunPod Secrets)
  GROK_API_KEY: "{{ RUNPOD_SECRET_GROK_API_KEY }}"
  
# Volume Mounts (for persistent storage)
volumes:
  - name: "models"
    mount_path: "/workspace/models"
    size_gb: 10
  - name: "data"
    mount_path: "/workspace/data"
    size_gb: 50

# Training Infrastructure Configuration
training:
  # Network Volume for training data
  # Complete pipeline requires ~140GB:
  #   - Raw datasets (AMASS, InterHuman, NTU-RGB+D, etc.): ~92 GB
  #   - Canonical/processed datasets: ~24 GB
  #   - Embedded training datasets: ~17 GB
  #   - Checkpoints (all 9 models): ~4 GB
  #   - Logs and intermediate files: ~3.5 GB
  #   - 2.5D Parallax PNG data (if generated): ~20-50 GB depending on views/frames
  network_volume:
    name: "stick-gen-training-data"
    size_gb: 250  # Recommended: 250GB for full pipeline with parallax + 30% buffer
    region: "US"  # Options: US, EU, etc.
    mount_path: "/runpod-volume"

  # Data paths (relative to mount_path)
  data_paths:
    amass: "/runpod-volume/data/amass"
    style_100: "/runpod-volume/data/100Style"
    smpl_models: "/runpod-volume/data/smpl_models"
    parallax: "/runpod-volume/data/2.5d_parallax"  # 2.5D parallax PNG + metadata

  # Training Pod configuration
  pod:
    name: "stick-gen-training"
    gpu_type: "NVIDIA RTX A5000"  # Cost-effective for small/medium models
    container_disk_gb: 50
    # GPU options by model size:
    # - small (5.6M): RTX 3060, RTX 3070, RTX A4000
    # - medium (15.8M): RTX 3090, RTX A5000, A100 40GB
    # - large (28M): A100 80GB, H100

  # 2.5D Parallax Augmentation Pipeline
  # Enabled by default in configs/*.yaml (data.use_parallax_augmentation: true)
  # Requires Node.js runtime and npm packages (three, pngjs, gl)
  parallax:
    enabled: true
    views_per_motion: 250      # Camera trajectories per motion sequence
    frames_per_view: 50        # PNG frames per camera trajectory (2 seconds at 25fps)
    image_size: [256, 256]     # [H, W] output PNG dimensions
    # Multi-frame sequence settings for temporal learning
    sequence_length: 25        # Frames per training sequence (1 second at 25fps)
    sequence_stride: 8         # Sliding window stride (~3-4 sequences per view)
    conditioning_mode: "first_frame"  # Options: first_frame, random_frame, all_frames
    # Storage estimate: ~150-300GB for 500k samples with 50 frames/view
    # CPU-bound (Node.js renderer), GPU optional for training

  # Multimodal Training Configuration
  # When data.use_parallax_augmentation: true in configs/*.yaml, the training
  # pipeline will use MultimodalParallaxDataset instead of StickFigureDataset.
  # This enables image-conditioned motion generation from 2.5D parallax frames.
  multimodal:
    # See docs/MODEL_SIZES.md for detailed parameter breakdowns
    # Image encoder architecture for processing parallax PNG frames
    # Options: lightweight_cnn (~2.1M params), resnet (~11.7M), mini_vit (~4-5M)
    image_encoder_arch: "lightweight_cnn"
    # Feature fusion strategy for combining text, image, and camera embeddings
    # Options: concat (~2.1M), gated (~2.4M, recommended), film (~2.2M), cross_attention (~15M)
    fusion_strategy: "gated"
    # Image loading backend
    # Options: pil (slower, more compatible), torchvision (faster, requires GPU)
    image_backend: "pil"
    # Total parameter overhead for multimodal (image encoder + fusion)
    # small/medium + lightweight_cnn + gated: +4.5M params
    # large + resnet + cross_attention: +26.7M params
    # Additional GPU VRAM required (approximate)
    # small/medium multimodal: +1-2GB, large multimodal: +4-6GB
    estimated_vram_overhead_gb: 2.0

  # Environment variables for training
  environment:
    MODEL_VARIANT: "${MODEL_VARIANT:-medium}"
    DATA_PATH: "/runpod-volume/data"
    # Optional: Path to the training dataset file consumed by src.train.train.
    # Defaults to "$DATA_PATH/train_data_final.pt" but can be pointed at a
    # curated, embedding-augmented dataset such as
    # "/runpod-volume/data/curated/pretrain_data_embedded.pt".
    TRAIN_DATA_PATH: "/runpod-volume/data/train_data_final.pt"
    AMASS_PATH: "/runpod-volume/data/amass"
    STYLE_PATH: "/runpod-volume/data/100Style"
    SMPL_PATH: "/runpod-volume/data/smpl_models"
    OUTPUT_PATH: "/runpod-volume/outputs"
    PARALLAX_ROOT: "/runpod-volume/data/2.5d_parallax"  # 2.5D parallax data
    DEVICE: "cuda"
    WANDB_MODE: "online"  # Set to "disabled" if not using W&B

  # Diffusion refinement settings (Phase 3)
  # The diffusion module now supports style conditioning from enhanced metadata
  # (tempo, energy_level, smoothness, valence, arousal) for metadata-aware refinement.
  diffusion:
    use_diffusion: true               # Enable diffusion refinement
    use_style_conditioning: true      # Enable style/emotion conditioning
    cfg_dropout_prob: 0.1             # Classifier-free guidance dropout during training
    guidance_scale: 2.0               # CFG scale for inference (>1 = stronger style adherence)
    diffusion_loss_weight: 0.1        # Weight for diffusion loss in total loss

# Network Settings
network:
  port: 8000
  timeout_seconds: 300     # 5 minute timeout for long generations

# Cost Optimization
cost:
  spot_instances: true     # Use spot instances for training
  auto_shutdown: true      # Shutdown when idle
  max_cost_per_hour: 2.0   # USD limit

# Model Variants (for reference)
variants:
  small:
    params: "5.6M"
    d_model: 256
    recommended_gpu: "RTX 3060"

  medium:
    params: "15.8M"
    d_model: 384
    recommended_gpu: "RTX 3090"

  large:
    params: "28M"
    d_model: 512
    recommended_gpu: "A100"

# API Examples
api_examples:
  generate:
    method: "POST"
    path: "/run"
    body:
      input:
        prompt: "A person walking confidently"
        num_frames: 60
        camera:
          x: 0.0
          y: 0.0
          zoom: 1.0
          
  stream:
    method: "GET"
    path: "/stream/{job_id}"
    description: "Stream results for long-running jobs"

