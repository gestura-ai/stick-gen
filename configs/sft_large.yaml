# Stick-Gen SFT Configuration - Large Variant
# Supervised Fine-Tuning on curated high-quality data
# Designed to be initialized from a pretrained checkpoint

# Model Variant Metadata
metadata:
  variant: "sft_large"
  display_name: "Stick-Gen Large (SFT)"
  expected_params: 28_000_000  # 28M base parameters
  recommended_hardware:
    cpu: "Not recommended"
    ram: "32GB+"
    gpu: "8GB+ VRAM (NVIDIA RTX 3060 or better)"
  use_cases:
    - "High-quality fine-tuning"
    - "Maximum animation quality"
    - "Production deployment"

# Model Architecture (matches large.yaml)
model:
  input_dim: 20
  d_model: 512
  nhead: 16
  num_layers: 10
  output_dim: 20
  embedding_dim: 1024
  dropout: 0.1
  num_actions: 64

# Training Settings (SFT-specific: lower LR, fewer epochs)
training:
  stage: "sft"              # Training stage identifier
  batch_size: 8             # Smaller than pretraining for stability
  grad_accum_steps: 8
  epochs: 25                # Fewer epochs than pretraining (100)
  learning_rate: 0.00005    # Lower LR for fine-tuning (5e-5 vs 5e-4)
  warmup_epochs: 3          # Shorter warmup
  max_grad_norm: 1.0
  resume_from: null         # For continuing SFT training
  init_from: null           # For initializing from pretrained checkpoint (weights only)

# LoRA Settings (optional, disabled by default)
lora:
  enabled: false            # Set to true to use LoRA fine-tuning
  rank: 16                  # Higher rank for larger model
  alpha: 32                 # Scaling factor (typically 2x rank)
  dropout: 0.05             # LoRA-specific dropout
  target_modules:           # Regex patterns for modules to adapt
    - "transformer_encoder"
    - "pose_decoder"

# Loss Weights (can emphasize action/physics for SFT)
loss_weights:
  temporal: 0.1
  action: 0.2               # Slightly higher for SFT
  physics: 0.25             # Slightly higher for SFT
  diffusion: 0.1

# Physics Settings
physics:
  enabled: true
  velocity_weight: 1.0
  acceleration_weight: 0.5
  momentum_weight: 0.3

# Diffusion Settings
diffusion:
  enabled: true             # Enable for large variant
  learning_rate: 0.0001
  timesteps: 1000

# Dataset Paths (SFT uses curated SFT dataset)
data:
  train_data: "data/curated/sft_data_embedded.pt"  # Default to SFT dataset
  curated_pretrain_data: "data/curated/pretrain_data_embedded.pt"
  curated_sft_data: "data/curated/sft_data_embedded.pt"
  checkpoint_dir: "checkpoints/sft_large"
  log_dir: "logs/sft_large"

# Data Generation Settings (inherited from large.yaml)
data_generation:
  num_samples: 100000
  output_path: "data/train_data.pt"
  embedded_path: "data/train_data_embedded.pt"
  merged_path: "data/train_data_merged.pt"
  augmentation:
    enabled: true
    multiplier: 4
  sequence:
    duration_seconds: 10.0
    fps: 25
    max_actors: 3
  llm:
    use_mock: true
    llm_ratio: 0.2

# Data Curation Settings (SFT uses strictest quality thresholds)
data_curation:
  # Merge step settings (merge_datasets.py)
  merge:
    balance_sources: true        # Enable source balancing
    max_source_fraction: 0.25    # Stricter: no source > 25% for large models
    filter_artifacts: true       # Remove artifacts during merge
    max_artifact_score: 0.3      # Strictest artifact threshold for quality
    min_frames: 25               # Min sequence length
    max_frames: 500              # Max sequence length
  # Curation step settings (prepare_curated_datasets.py)
  curation:
    min_quality_pretrain: 0.5    # Not used for SFT
    min_quality_sft: 0.85        # Highest quality bar for large model SFT
    min_camera_stability_sft: 0.65
    balance_max_fraction: 0.25   # Strictest action balance for large model

# Device Settings
device:
  type: "cuda"              # GPU required for large model
  num_workers: 4
  pin_memory: true

# Logging Settings
logging:
  level: "INFO"
  log_interval: 10
  save_interval: 5
  verbose_file: "training_verbose.log"

# Optimization
optimization:
  optimizer: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# CPU Optimization (not used for GPU)
cpu:
  num_threads: 4
  use_mkldnn: false

