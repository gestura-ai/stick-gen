# Stick-Gen Training Configuration - CPU Optimized
# Optimized for affordable CPU training with limited memory (8-16GB RAM)

# Model Variant Metadata
metadata:
  variant: "small"
  display_name: "Stick-Gen Small"
  expected_params: 7_200_000  # 7.2M parameters (SwiGLU + RMSNorm architecture)
  recommended_hardware:
    cpu: "4+ cores"
    ram: "8-16GB"
    gpu: "Not required"
  inference:
    batch_size: 1
    avg_latency_ms: 2000  # Approximate on 4-core CPU
    throughput_fps: 0.5   # Sequences per second
  use_cases:
    - "Budget CPU deployment"
    - "Edge devices"
    - "Development and testing"
    - "Low-resource environments"

# Model Architecture (smaller for CPU)
# Uses modern LLM architecture: RMSNorm + SwiGLU + Pre-Norm
model:
  input_dim: 20
  d_model: 256            # Reduced from 384 (7.2M params instead of 20.5M)
  nhead: 8                # Reduced from 12
  num_layers: 6           # Reduced from 8
  output_dim: 20
  embedding_dim: 1024
  dropout: 0.1
  num_actions: 64

# Training Settings (CPU-friendly)
training:
  batch_size: 1           # Minimal batch size for low memory
  grad_accum_steps: 64    # Higher accumulation to maintain effective batch size
  epochs: 30              # Fewer epochs for faster training
  learning_rate: 0.0003
  warmup_epochs: 5        # Shorter warmup
  max_grad_norm: 1.0
  resume_from: null       # Optional: checkpoint path for continued pretraining

# Loss Weights
loss_weights:
  temporal: 0.1
  action: 0.15
  physics: 0.2
  diffusion: 0.0          # Disabled for CPU
  diff_physics: 0.1       # Differentiable physics loss weight (Brax)

# Physics Settings
physics:
  enabled: true
  velocity_weight: 1.0
  acceleration_weight: 0.5
  momentum_weight: 0.3

# Diffusion Settings
diffusion:
  enabled: false          # Disabled for CPU training

# LoRA Settings (Low-Rank Adaptation)
lora:
  enabled: true           # Enable LoRA for efficient fine-tuning
  rank: 8                 # Rank of LoRA adapters
  alpha: 16.0             # LoRA alpha scaling
  dropout: 0.05           # LoRA dropout
  target_modules: ["transformer_encoder", "pose_decoder"]


# Dataset Paths
data:
  train_data: "data/train_data_final.pt"               # Legacy combined dataset (embedding-augmented)
  curated_pretrain_data: "data/curated/pretrain_data_embedded.pt"  # Curated pretraining dataset (with embeddings)
  curated_sft_data: "data/curated/sft_data_embedded.pt"            # Curated SFT dataset (with embeddings)
  checkpoint_dir: "checkpoints"
  log_dir: "logs"

# Data Generation Settings
data_generation:
  num_samples: 10000           # Smaller dataset for quick iteration
  output_path: "data/train_data.pt"
  embedded_path: "data/train_data_embedded.pt"
  merged_path: "data/train_data_merged.pt"
  augmentation:
    enabled: true
    multiplier: 4
  sequence:
    duration_seconds: 10.0
    fps: 25
    max_actors: 3
  llm:
    use_mock: true
    llm_ratio: 0.2

# Data Curation Settings (for curated pipeline)
data_curation:
  # Merge step settings (merge_datasets.py)
  merge:
    balance_sources: true        # Enable source balancing
    max_source_fraction: 0.35    # Allow slightly higher per-source for small datasets
    filter_artifacts: true       # Remove artifacts during merge
    max_artifact_score: 0.4      # Max artifact score threshold
    min_frames: 25               # Min sequence length
    max_frames: 500              # Max sequence length
  # Curation step settings (prepare_curated_datasets.py)
  curation:
    min_quality_pretrain: 0.5    # Lower bar for pretraining
    min_quality_sft: 0.8         # Higher bar for SFT
    min_camera_stability_sft: 0.6
    balance_max_fraction: 0.3    # Max fraction per action in SFT

# Device Settings
device:
  type: "cpu"
  num_workers: 0
  pin_memory: false

# Logging Settings
logging:
  level: "INFO"
  log_interval: 20        # Less frequent logging
  save_interval: 10       # Save less frequently
  verbose_file: "training_verbose.log"

# Optimization
optimization:
  optimizer: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# CPU Optimization
cpu:
  num_threads: 8          # Adjust based on your CPU
  use_mkldnn: true

