# Stick-Gen Training Configuration - Medium Variant
# Recommended default configuration for balanced quality and performance

# Model Variant Metadata
metadata:
  variant: "medium"
  display_name: "Stick-Gen Medium"
  expected_params: 20_500_000  # 20.5M parameters (SwiGLU + RMSNorm architecture)
  recommended_hardware:
    cpu: "8+ cores"
    ram: "16-32GB"
    gpu: "8GB+ VRAM recommended"
  inference:
    batch_size: 4
    avg_latency_ms: 500  # Approximate on RTX 3090
    throughput_fps: 2.0  # Sequences per second
  use_cases:
    - "Recommended default deployment"
    - "Research and development"
    - "Production with moderate resources"
    - "Balanced quality and performance"

# Model Architecture
# Uses modern LLM architecture: RMSNorm + SwiGLU + Pre-Norm
model:
  input_dim: 20           # 10 joints × 2 coords
  d_model: 384            # Transformer hidden dimension (384 = 20.5M params)
  nhead: 12               # Number of attention heads
  num_layers: 8           # Number of transformer layers
  output_dim: 20          # Same as input_dim
  embedding_dim: 1024     # Text embedding dimension (BAAI/bge-large-en-v1.5)
  dropout: 0.1            # Dropout rate
  num_actions: 64         # Number of action classes

# Training Settings
training:
  batch_size: 2           # Batch size (reduce if OOM)
  grad_accum_steps: 32    # Gradient accumulation steps (effective batch = batch_size × grad_accum_steps)
  epochs: 50              # Number of training epochs
  learning_rate: 0.0003   # Initial learning rate
  warmup_epochs: 10       # Number of warmup epochs
  max_grad_norm: 1.0      # Gradient clipping threshold
  resume_from: null       # Optional: checkpoint path for continued pretraining/resume
  
# Loss Weights
loss_weights:
  temporal: 0.1           # Temporal consistency loss weight
  action: 0.15            # Action prediction loss weight
  physics: 0.2            # Physics loss weight
  diff_physics: 0.1       # Differentiable physics loss weight (Brax)
  diffusion: 0.1          # Diffusion refinement loss weight (if enabled)

# Physics Settings
physics:
  enabled: true           # Enable physics-aware training
  velocity_weight: 1.0    # Velocity loss weight
  acceleration_weight: 0.5  # Acceleration loss weight
  momentum_weight: 0.3    # Momentum loss weight

# Diffusion Settings
diffusion:
  enabled: true          # Enable diffusion refinement (experimental)
  learning_rate: 0.0001   # Diffusion model learning rate
  timesteps: 1000         # Number of diffusion timesteps

# LoRA Settings (Low-Rank Adaptation)
lora:
  enabled: true           # Enable LoRA for efficient fine-tuning
  rank: 8                 # Rank of LoRA adapters
  alpha: 16.0             # LoRA alpha scaling
  dropout: 0.05           # LoRA dropout
  target_modules: ["transformer_encoder", "pose_decoder"]


# Dataset Paths
data:
  train_data: "data/train_data_final.pt"                    # Legacy combined dataset (embedding-augmented)
  curated_pretrain_data: "data/curated/pretrain_data_embedded.pt"  # Curated pretraining dataset (with embeddings)
  curated_sft_data: "data/curated/sft_data_embedded.pt"            # Curated SFT dataset (with embeddings)
  checkpoint_dir: "checkpoints"                                # Directory for saving checkpoints
  log_dir: "logs"                                              # Directory for training logs

# Data Generation Settings
data_generation:
  num_samples: 50000           # Number of base samples to generate
  output_path: "data/train_data.pt"  # Raw generated data output
  embedded_path: "data/train_data_embedded.pt"  # With text embeddings
  merged_path: "data/train_data_merged.pt"  # Merged with AMASS data
  augmentation:
    enabled: true              # Enable data augmentation
    multiplier: 4              # Augmentation multiplier (4x = speed, position, scale, mirror)
  sequence:
    duration_seconds: 10.0     # Sequence duration in seconds
    fps: 25                    # Frames per second
    max_actors: 3              # Maximum actors per scene
  llm:
    use_mock: true             # Use mock LLM (set false to enable real Grok API)
                               # When false, requires GROK_API_KEY in .env file
                               # See GROK_API_FIX_SUMMARY.md for setup instructions
    llm_ratio: 0.2             # Ratio of LLM-generated scenes (0.0 to 1.0)
                               # 0.2 = 20% of samples use Grok API
                               # Higher values = more AI variety but higher API costs
                               # Recommended: 0.1-0.3 for good balance

# Data Curation Settings (for curated pipeline)
data_curation:
  # Merge step settings (merge_datasets.py)
  merge:
    balance_sources: true        # Enable source balancing
    max_source_fraction: 0.3     # Default: no source > 30%
    filter_artifacts: true       # Remove artifacts during merge
    max_artifact_score: 0.4      # Max artifact score threshold
    min_frames: 25               # Min sequence length
    max_frames: 500              # Max sequence length
  # Curation step settings (prepare_curated_datasets.py)
  curation:
    min_quality_pretrain: 0.5    # Lower bar for pretraining
    min_quality_sft: 0.8         # Higher bar for SFT
    min_camera_stability_sft: 0.6
    balance_max_fraction: 0.3    # Max fraction per action in SFT

# Device Settings
device:
  type: "auto"            # "auto", "cpu", "cuda", or "mps"
  num_workers: 0          # DataLoader workers (0 for CPU, 4+ for GPU)
  pin_memory: false       # Pin memory for faster GPU transfer

# Logging Settings
logging:
  level: "INFO"           # Logging level: "DEBUG", "INFO", "WARNING", "ERROR"
  log_interval: 10        # Log every N gradient steps
  save_interval: 5        # Save checkpoint every N epochs
  verbose_file: "training_verbose.log"  # Verbose log file path

# Optimization
optimization:
  optimizer: "AdamW"      # Optimizer type
  weight_decay: 0.01      # Weight decay for regularization
  betas: [0.9, 0.999]     # Adam beta parameters
  eps: 1.0e-8             # Adam epsilon

# CPU Optimization (when device.type == "cpu")
cpu:
  num_threads: 8          # Number of CPU threads
  use_mkldnn: true        # Use MKL-DNN optimization

