# Stick-Gen Model Sizes Reference

> **Single Source of Truth** for model parameter counts.
>
> To verify: `python scripts/verify_model_params.py`
> To regenerate detailed breakdown: `python scripts/calculate_model_params.py --markdown`

## Summary

| Variant | Motion-Only | Multimodal | Overhead | Image Encoder | Fusion Strategy |
|---------|-------------|------------|----------|---------------|-----------------|
| **Small** | 7.2M (7,248,122) | 11.7M (11,717,412) | +4.5M | lightweight_cnn | gated |
| **Medium** | 20.6M (20,594,618) | 25.1M (25,063,908) | +4.5M | lightweight_cnn | gated |
| **Large** | 44.6M (44,618,362) | 71.3M (71,282,874) | +26.7M | resnet | cross_attention |

### When to Use Each Variant

| Variant | Use Case | Hardware Requirements |
|---------|----------|----------------------|
| **Small** (7.2M/11.7M) | Edge devices, testing, budget deployments | CPU (4+ cores), 2GB RAM |
| **Medium** (20.6M/25.1M) | **Recommended default**, balanced quality | CPU/GPU, 8GB RAM |
| **Large** (44.6M/71.3M) | Maximum quality, production workloads | GPU (8GB+ VRAM) |

## Architecture Configuration

| Variant | d_model | Layers | Heads | Embedding Dim |
|---------|---------|--------|-------|---------------|
| Small | 256 | 6 | 8 | 1024 |
| Medium | 384 | 8 | 12 | 1024 |
| Large | 512 | 10 | 16 | 1024 |

## Multimodal Components

When `data.use_parallax_augmentation: true`, the following components are added:

### Image Encoder Options

| Architecture | Parameters | Config Value | Recommended For |
|--------------|------------|--------------|-----------------|
| Lightweight CNN | ~2.1M | `lightweight_cnn` | Small/Medium models |
| ResNet | ~11.7M | `resnet` | Large model |
| Mini ViT | ~4-5M | `mini_vit` | Experimental |

### Fusion Strategy Options

| Strategy | Parameters | Config Value | Description |
|----------|------------|--------------|-------------|
| Gated | ~2.4M | `gated` | Learned modality weights (recommended) |
| Concat | ~2.1M | `concat` | Simple concatenation + projection |
| FiLM | ~2.2M | `film` | Feature-wise Linear Modulation |
| Cross-Attention | ~15M | `cross_attention` | Most expressive (large model) |

## Detailed Component Breakdown

### Small Variant (d=256, L=6, H=8)

| Component | Motion-Only | Multimodal |
|-----------|-------------|------------|
| Transformer Encoder | 6.3M | 6.3M |
| Text Projection | 656.6K | 656.6K |
| Image Encoder | - | 2.1M |
| Fusion Module | - | 2.4M |
| Decoder Heads | ~200K | ~200K |
| **Total** | **7.2M** | **11.7M** |

### Medium Variant (d=384, L=8, H=12)

| Component | Motion-Only | Multimodal |
|-----------|-------------|------------|
| Transformer Encoder | 18.9M | 18.9M |
| Text Projection | 1.1M | 1.1M |
| Image Encoder | - | 2.1M |
| Fusion Module | - | 2.4M |
| Decoder Heads | ~475K | ~475K |
| **Total** | **20.6M** | **25.1M** |

### Large Variant (d=512, L=10, H=16)

| Component | Motion-Only | Multimodal |
|-----------|-------------|------------|
| Transformer Encoder | 42.0M | 42.0M |
| Text Projection | 1.6M | 1.6M |
| Image Encoder (ResNet) | - | 11.7M |
| Fusion Module (Cross-Attn) | - | 15.0M |
| Decoder Heads | ~850K | ~850K |
| **Total** | **44.6M** | **71.3M** |

## Estimated File Sizes

| Variant | Motion-Only (FP32) | Motion-Only (FP16) | Multimodal (FP32) | Multimodal (FP16) |
|---------|-------------------|-------------------|-------------------|-------------------|
| Small | ~29 MB | ~15 MB | ~47 MB | ~24 MB |
| Medium | ~82 MB | ~41 MB | ~100 MB | ~50 MB |
| Large | ~178 MB | ~89 MB | ~285 MB | ~143 MB |

## Memory Requirements (Training)

| Variant | Motion-Only | Multimodal | Batch Size=32 | Notes |
|---------|-------------|------------|---------------|-------|
| Small | ~2 GB | ~3 GB | ~4 GB | CPU feasible |
| Medium | ~4 GB | ~6 GB | ~8 GB | GPU recommended |
| Large | ~8 GB | ~14 GB | ~20 GB | GPU required |

---

*Last updated: Generated by `scripts/calculate_model_params.py`*

