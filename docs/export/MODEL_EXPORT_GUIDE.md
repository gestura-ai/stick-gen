# Modern Model Export Formats Guide

Complete guide to exporting stick-gen models using industry-standard formats for maximum compatibility, security, and deployment flexibility.

---

## üìã **Overview**

**Current State**: PyTorch `.pth` files (pickle-based, security risks, PyTorch-only)  
**Target State**: Multi-format support (Hugging Face, ONNX, Safetensors)

**Model**: StickFigureTransformer (15.5M parameters)  
**Use Cases**: Sharing, deployment, model hubs, cross-platform inference

---

## üéØ **Recommended Formats**

### **1. Hugging Face Format (PRIMARY - Industry Standard)** ‚≠ê‚≠ê‚≠ê

**Used by**: GPT-2, BERT, Llama, Mistral, Stable Diffusion, all modern LLMs

**Components**:
- `model.safetensors` - Secure weight storage (no pickle, no code execution)
- `config.json` - Model architecture configuration
- `README.md` - Model card with documentation, usage, metrics
- `tokenizer_config.json` (optional) - For text models

**Why Better than .pth**:
- ‚úÖ **Security**: Safetensors cannot execute arbitrary code (unlike pickle)
- ‚úÖ **Speed**: 2-3x faster loading than pickle
- ‚úÖ **Memory**: Zero-copy loading, lower memory footprint
- ‚úÖ **Compatibility**: Works with Hugging Face Hub, Transformers library
- ‚úÖ **Discoverability**: Model cards make models searchable and documented
- ‚úÖ **Versioning**: Easy to track model versions and updates

**Industry Adoption**:
- Hugging Face Hub: 500k+ models use this format
- Stability AI: All Stable Diffusion models
- Meta: Llama 2, Llama 3
- Mistral AI: All Mistral models
- Anthropic: Claude model exports

**File Structure**:
```
stick-gen-model/
‚îú‚îÄ‚îÄ model.safetensors          # Model weights (secure)
‚îú‚îÄ‚îÄ config.json                # Architecture config
‚îú‚îÄ‚îÄ README.md                  # Model card
‚îú‚îÄ‚îÄ training_args.json         # Training hyperparameters
‚îî‚îÄ‚îÄ generation_config.json     # Generation settings (optional)
```

---

### **2. ONNX Format (SECONDARY - Cross-Platform Deployment)** ‚≠ê‚≠ê

**Used by**: Production deployments, edge devices, cross-platform inference

**Components**:
- `model.onnx` - ONNX graph representation
- `config.json` - Model metadata

**Why Better than .pth**:
- ‚úÖ **Platform-agnostic**: Works with TensorRT, ONNX Runtime, CoreML, TensorFlow
- ‚úÖ **Optimized**: Graph optimizations for faster inference
- ‚úÖ **Hardware acceleration**: Easy integration with GPUs, TPUs, NPUs
- ‚úÖ **Mobile/Edge**: Deploy to iOS, Android, embedded systems
- ‚úÖ **Language-agnostic**: Use from C++, C#, Java, JavaScript

**Deployment Targets**:
- ONNX Runtime (CPU/GPU)
- TensorRT (NVIDIA GPUs)
- CoreML (Apple devices)
- OpenVINO (Intel hardware)
- DirectML (Windows)

**Use Cases**:
- Production web services
- Mobile apps (iOS/Android)
- Edge devices (Raspberry Pi, Jetson)
- Browser inference (ONNX.js)

---

### **3. TorchScript Format (OPTIONAL - PyTorch Production)** ‚≠ê

**Used by**: PyTorch production deployments, C++ inference

**Components**:
- `model.pt` - TorchScript serialized model
- `config.json` - Model metadata

**Why Better than .pth**:
- ‚úÖ **No Python dependency**: Run in C++ environments
- ‚úÖ **Optimized**: JIT compilation for faster inference
- ‚úÖ **Mobile**: PyTorch Mobile for iOS/Android
- ‚úÖ **Deployment**: Easier production deployment

**Use Cases**:
- C++ inference servers
- PyTorch Mobile apps
- Environments without Python

---

### **4. Safetensors Only (ALTERNATIVE - Lightweight)** ‚≠ê‚≠ê

**Used by**: When you want security without full Hugging Face integration

**Components**:
- `model.safetensors` - Just the weights
- `config.json` - Architecture config

**Why Better than .pth**:
- ‚úÖ **Security**: No pickle vulnerabilities
- ‚úÖ **Speed**: Fast loading
- ‚úÖ **Simple**: Minimal dependencies

**Use Cases**:
- Internal model sharing
- Security-conscious environments
- When you don't need Hugging Face Hub

---

## üìä **Format Comparison**

| Format | Security | Speed | Compatibility | Deployment | Hub Integration |
|--------|----------|-------|---------------|------------|-----------------|
| **.pth (current)** | ‚ùå Low | ‚ö†Ô∏è Medium | PyTorch only | Limited | None |
| **Hugging Face** | ‚úÖ High | ‚úÖ Fast | Excellent | Good | ‚úÖ Full |
| **ONNX** | ‚úÖ High | ‚úÖ Very Fast | Universal | ‚úÖ Excellent | Partial |
| **TorchScript** | ‚úÖ High | ‚úÖ Fast | PyTorch | Good | None |
| **Safetensors** | ‚úÖ High | ‚úÖ Fast | Good | Good | Partial |

---

## üöÄ **Recommended Strategy for Stick-Gen**

### **Phase 1: Hugging Face Format (PRIMARY)**
Export all models to Hugging Face format for:
- Sharing on Hugging Face Hub
- Documentation and discoverability
- Security and speed improvements
- Community adoption

### **Phase 2: ONNX Export (DEPLOYMENT)**
Export to ONNX for:
- Production web services
- Cross-platform deployment
- Hardware acceleration
- Mobile/edge deployment

### **Phase 3: Keep .pth (BACKWARD COMPATIBILITY)**
Maintain .pth support during transition:
- Existing checkpoints still work
- Gradual migration
- No breaking changes

---

## üì¶ **What Changes Are Needed**

### **1. Dependencies**
```bash
pip install safetensors huggingface-hub onnx onnxruntime
```

### **2. Export Scripts**
Create `export_model.py` to convert .pth ‚Üí all formats

### **3. Inference Updates**
Update `src/inference/generator.py` to support multiple formats:
```python
# Auto-detect format and load accordingly
if path.endswith('.safetensors'):
    load_safetensors(path)
elif path.endswith('.onnx'):
    load_onnx(path)
elif path.endswith('.pth'):
    load_pth(path)  # Backward compatibility
```

### **4. Model Card**
Create `README.md` with:
- Model description
- Training details
- Usage examples
- Performance metrics
- Limitations

---

## üîÑ **Migration Path**

### **Step 1: Export Existing Checkpoints**
```bash
python export_model.py \
    --input checkpoint_epoch_50.pth \
    --output stick-gen-v1 \
    --formats safetensors onnx
```

### **Step 2: Update Inference Code**
Add multi-format loading support

### **Step 3: Test Compatibility**
Verify all formats produce identical outputs

### **Step 4: Publish to Hub**
```bash
huggingface-cli login
python export_model.py --push-to-hub gestura-ai/stick-gen-v1
```

### **Step 5: Deprecate .pth (Optional)**
After transition period, make Hugging Face format primary

---

**Next**: See `export_model.py` for implementation details.

